{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tuz6OWV4_6H"
   },
   "source": [
    "### 豆瓣评分的预测\n",
    "\n",
    "在这个项目中，我们要预测一部电影的评分，这个问题实际上就是一个分类问题。给定的输入为一段文本，输出为具体的评分。 在这个项目中，我们需要做：\n",
    "- 文本的预处理，如停用词的过滤，低频词的过滤，特殊符号的过滤等\n",
    "- 文本转化成向量，将使用三种方式，分别为tf-idf, word2vec以及BERT向量。 \n",
    "- 训练逻辑回归和朴素贝叶斯模型，并做交叉验证\n",
    "- 评估模型的准确率\n",
    "\n",
    "在具体标记为``TODO``的部分填写相应的代码。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etX7QtT8534P",
    "outputId": "30a7be04-c838-4a8d-f639-c6978ef5b3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbY3Fph6U-xU"
   },
   "outputs": [],
   "source": [
    "#pip install bert_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCwGa2TCSnEF"
   },
   "outputs": [],
   "source": [
    "pip install mxnet-cu100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9D3G81Hm4_6L"
   },
   "outputs": [],
   "source": [
    "\n",
    "#导入数据处理的基础包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#导入用于计数的包\n",
    "from collections import Counter\n",
    "\n",
    "#导入tf-idf相关的包\n",
    "from sklearn.feature_extraction.text import TfidfTransformer    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#导入模型评估的包\n",
    "from sklearn import metrics\n",
    "\n",
    "#导入与word2vec相关的包\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#导入与bert embedding相关的包，关于mxnet包下载的注意事项参考实验手册\n",
    "from bert_embedding import BertEmbedding\n",
    "import mxnet\n",
    "from mxnet import gpu\n",
    "import jieba\n",
    "#包tqdm是用来对可迭代对象执行时生成一个进度条用以监视程序运行过程\n",
    "from tqdm import tqdm\n",
    "\n",
    "#导入其他一些功能包\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rViLZrex4_6N"
   },
   "source": [
    "### 1. 读取数据并做文本的处理\n",
    "你需要完成以下几步操作：\n",
    "- 去掉无用的字符如！&，可自行定义\n",
    "- 中文分词\n",
    "- 去掉低频词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRZ-dFZo4_6N"
   },
   "outputs": [],
   "source": [
    "#读取数据\n",
    "#data = pd.read_csv('/content/drive/MyDrive/Greedy/project1-douban/data/DMSC.csv')\n",
    "data = pd.read_csv('data/DMSC.csv')\n",
    "#观察数据格式\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8spVjo4vGGrp"
   },
   "outputs": [],
   "source": [
    "data['34'，]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siwwa-DT4_6N"
   },
   "outputs": [],
   "source": [
    "#输出数据的一些相关信息\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vva8ffos4_6N"
   },
   "outputs": [],
   "source": [
    "#只保留数据中我们需要的两列：Comment列和Star列\n",
    "data = data[['Comment','Star']]\n",
    "#观察新的数据的格式\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "7oyt1dfr4_6O",
    "outputId": "252dec93-6394-4156-9b68-f7290578ee66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>连奥创都知道整容要去韩国。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“一个没有黑暗面的人不值得信任。” 第二部剥去冗长的铺垫，开场即高潮、一直到结束，会有人觉...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊！！！！！！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>与第一集不同，承上启下，阴郁严肃，但也不会不好看啊，除非本来就不喜欢漫威电影。场面更加宏大...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看毕，我激动地对友人说，等等奥创要来毁灭台北怎么办厚，她拍了拍我肩膀，没事，反正你买了两份...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Star\n",
       "0                                      连奥创都知道整容要去韩国。     1\n",
       "1   “一个没有黑暗面的人不值得信任。” 第二部剥去冗长的铺垫，开场即高潮、一直到结束，会有人觉...     1\n",
       "2                                 奥创弱爆了弱爆了弱爆了啊！！！！！！     0\n",
       "3   与第一集不同，承上启下，阴郁严肃，但也不会不好看啊，除非本来就不喜欢漫威电影。场面更加宏大...     1\n",
       "4   看毕，我激动地对友人说，等等奥创要来毁灭台北怎么办厚，她拍了拍我肩膀，没事，反正你买了两份...     1"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里的star代表具体的评分。但在这个项目中，我们要预测的是正面还是负面。我们把评分为1和2的看作是负面，把评分为3，4，5的作为正面\n",
    "data['Star']=(data.Star/3).astype(int)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCy96Pg-4_6O"
   },
   "source": [
    "#### 任务1： 去掉一些无用的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDlaQEpK4_6O",
    "outputId": "a95cf772-45e1-4a76-ed4a-f3e995a603e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "apply: 100%|██████████| 212506/212506 [00:01<00:00, 127828.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO1: 去掉一些无用的字符，自行定一个字符几何，并从文本中去掉\n",
    "#    your to do \n",
    "import re \n",
    "def clear_character(sentence):\n",
    "    pattern1='[a-zA-Z0-9]'\n",
    "    pattern2 = re.compile(u'[^\\s1234567890:：' + '\\u4e00-\\u9fa5]+')\n",
    "    pattern3='[’!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+'\n",
    "    line1=re.sub(pattern1,'',sentence)   #去除英文字母和数字\n",
    "    line2=re.sub(pattern2,'',line1)   #去除表情和其他字符\n",
    "    line3=re.sub(pattern3,'',line2)   #去除去掉残留的冒号及其它符号\n",
    "    new_sentence=''.join(line3.split()) #去除空白\n",
    "    return new_sentence\n",
    "\n",
    "tqdm.pandas(desc='apply')\n",
    "data['Comment'] = data['Comment'].progress_apply(clear_character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cfrVTJFBWAy"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjogx6eN4_6O"
   },
   "source": [
    "#### 任务2：使用结巴分词对文本做分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_d5J2_P4_6P",
    "outputId": "8649b76f-2c7e-491e-f1b0-62af5fe2375e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "\r",
      "apply:   0%|          | 0/212506 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.745 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "apply: 100%|██████████| 212506/212506 [00:31<00:00, 6776.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO2: 导入中文分词包jieba, 并用jieba对原始文本做分词\n",
    "def comment_cut(content):\n",
    "    # TODO: 使用结巴完成对每一个comment的分词\n",
    "    document_cut = jieba.cut(content)\n",
    "    word = []\n",
    "    for i in document_cut:\n",
    "      word.append(i)\n",
    "    \n",
    "    return word\n",
    "\n",
    "# 输出进度条\n",
    "tqdm.pandas(desc='apply')\n",
    "data['comment_processed'] = data['Comment'].progress_apply(comment_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "S0b4iWfO4_6P",
    "outputId": "5bf63dbe-e154-462e-8d12-978315e082e7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "      <th>comment_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>连奥创都知道整容要去韩国</td>\n",
       "      <td>1</td>\n",
       "      <td>[连, 奥创, 都, 知道, 整容, 要, 去, 韩国]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...</td>\n",
       "      <td>1</td>\n",
       "      <td>[一个, 没有, 黑暗面, 的, 人, 不, 值得, 信任, 第二部, 剥去, 冗长, 的,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊</td>\n",
       "      <td>0</td>\n",
       "      <td>[奥创, 弱, 爆, 了, 弱, 爆, 了, 弱, 爆, 了, 啊]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...</td>\n",
       "      <td>1</td>\n",
       "      <td>[与, 第一集, 不同, 承上启下, 阴郁, 严肃, 但, 也, 不会, 不, 好看, 啊,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹</td>\n",
       "      <td>1</td>\n",
       "      <td>[看毕, 我, 激动, 地, 对, 友人, 说, 等等, 奥创, 要, 来, 毁灭, 台北,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  ...                                  comment_processed\n",
       "0                                       连奥创都知道整容要去韩国  ...                       [连, 奥创, 都, 知道, 整容, 要, 去, 韩国]\n",
       "1  一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...  ...  [一个, 没有, 黑暗面, 的, 人, 不, 值得, 信任, 第二部, 剥去, 冗长, 的,...\n",
       "2                                       奥创弱爆了弱爆了弱爆了啊  ...                 [奥创, 弱, 爆, 了, 弱, 爆, 了, 弱, 爆, 了, 啊]\n",
       "3  与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...  ...  [与, 第一集, 不同, 承上启下, 阴郁, 严肃, 但, 也, 不会, 不, 好看, 啊,...\n",
       "4      看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹  ...  [看毕, 我, 激动, 地, 对, 友人, 说, 等等, 奥创, 要, 来, 毁灭, 台北,...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 观察新的数据的格式\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eVDqFFA4_6P"
   },
   "source": [
    "#### 任务3：设定停用词并去掉停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbUf7yMA4_6P",
    "outputId": "df856e8b-5e9f-4854-c21f-fc2f0efab63b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "apply: 100%|██████████| 212506/212506 [00:43<00:00, 4850.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO3: 设定停用词并从文本中去掉停用词\n",
    "\n",
    "# 下载中文停用词表至data/stopWord.json中，下载地址:https://github.com/goto456/stopwords/\n",
    "#if not os.path.exists('/content/drive/MyDrive/Greedy/project1-douban/data/stopWord.json'):\n",
    "#    stopWord = requests.get(\"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\")\n",
    "#    with open(\"data/stopWord.json\", \"wb\") as f:\n",
    "#         f.write(stopWord.content)\n",
    "\n",
    "# 读取下载的停用词表，并保存在列表中\n",
    "#with open(\"data/stopWord.json\",\"r\") as f:\n",
    "with open(\"/content/drive/MyDrive/Greedy/project1-douban/data/stopWord.json\",\"r\") as f:    \n",
    "    stopWords = f.read().split(\"\\n\")  \n",
    "    \n",
    "    \n",
    "# 去除停用词\n",
    "def rm_stop_word(wordList):\n",
    "    # your code, remove stop words\n",
    "    tokenized_com = [w for w in wordList if not w in stopWords]\n",
    "    return tokenized_com\n",
    "\n",
    "#这行代码中.progress_apply()函数的作用等同于.apply()函数的作用，只是写成.progress_apply()函数才能被tqdm包监控从而输出进度条。\n",
    "tqdm.pandas(desc='apply')\n",
    "data['comment_processed'] = data['comment_processed'].progress_apply(rm_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "Rw9s7tuJ4_6Q",
    "outputId": "66ec3fd7-94b1-42a1-cb15-e5b2ae69091f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "      <th>comment_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>连奥创都知道整容要去韩国</td>\n",
       "      <td>1</td>\n",
       "      <td>[奥创, 知道, 整容, 韩国]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...</td>\n",
       "      <td>1</td>\n",
       "      <td>[一个, 没有, 黑暗面, 值得, 信任, 第二部, 剥去, 冗长, 铺垫, 开场, 高潮,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊</td>\n",
       "      <td>0</td>\n",
       "      <td>[奥创, 弱, 爆, 弱, 爆, 弱, 爆]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...</td>\n",
       "      <td>1</td>\n",
       "      <td>[第一集, 不同, 承上启下, 阴郁, 严肃, 不会, 好看, 本来, 喜欢, 漫威, 电影...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹</td>\n",
       "      <td>1</td>\n",
       "      <td>[看毕, 激动, 友人, 说, 奥创, 毁灭, 台北, 厚, 拍了拍, 肩膀, 没事, 反正...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  ...                                  comment_processed\n",
       "0                                       连奥创都知道整容要去韩国  ...                                   [奥创, 知道, 整容, 韩国]\n",
       "1  一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...  ...  [一个, 没有, 黑暗面, 值得, 信任, 第二部, 剥去, 冗长, 铺垫, 开场, 高潮,...\n",
       "2                                       奥创弱爆了弱爆了弱爆了啊  ...                             [奥创, 弱, 爆, 弱, 爆, 弱, 爆]\n",
       "3  与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...  ...  [第一集, 不同, 承上启下, 阴郁, 严肃, 不会, 好看, 本来, 喜欢, 漫威, 电影...\n",
       "4      看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹  ...  [看毕, 激动, 友人, 说, 奥创, 毁灭, 台北, 厚, 拍了拍, 肩膀, 没事, 反正...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 观察新的数据的格式\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CKuGNMh4_6Q"
   },
   "source": [
    "#### 任务4：去掉低频词，出现次数少于10次的词去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGGNFqy64_6Q",
    "outputId": "06299476-620c-4942-cf55-d637eb4f20c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212506/212506 [11:16<00:00, 314.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO4: 去除低频词, 去掉词频小于10的单词，\n",
    "from collections import Counter\n",
    "#统计词频\n",
    "word_list = Counter()\n",
    "\n",
    "for i in tqdm(range(len(data['comment_processed']))):\n",
    "  tmp = Counter(data['comment_processed'][i])\n",
    "  word_list+=tmp\n",
    "\n",
    "#data['comment_processed'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afZxCZmUJnDh",
    "outputId": "4eedf2ab-7966-41bf-89ab-9eaa2c4117f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212506/212506 [00:01<00:00, 193647.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "171880"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#收集低频词\n",
    "low_fre = []\n",
    "for i in tqdm(range(len(data['comment_processed']))):\n",
    "  for term in data['comment_processed'][i]:\n",
    "    if word_list[term] < 10:\n",
    "      low_fre.append(term)\n",
    "len(low_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEZAeZq0nh8_",
    "outputId": "50bbe901-6517-461d-ee0d-6912b3a5aad9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bin/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "apply: 100%|██████████| 212506/212506 [6:19:29<00:00,  9.33it/s]   \n"
     ]
    }
   ],
   "source": [
    "#去除低频词 并把结果存放在data['comment_processed']里\n",
    "def rm_low_fre_word(wordList):\n",
    "\n",
    "    tokenized_com = [w for w in wordList if not w in low_fre]\n",
    "    return tokenized_com\n",
    "\n",
    "tqdm.pandas(desc='apply')\n",
    "data['comment_processed'] = data['comment_processed'].progress_apply(rm_low_fre_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "l2YunOSKotbr",
    "outputId": "1ff5232b-7764-4e22-b4d2-ebdba61e632c",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "      <th>comment_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>连奥创都知道整容要去韩国</td>\n",
       "      <td>1</td>\n",
       "      <td>奥创 知道 整容 韩国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...</td>\n",
       "      <td>1</td>\n",
       "      <td>一个 没有 黑暗面 值得 信任 第二部 冗长 铺垫 开场 高潮 一直 结束 会 有人 觉得 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊</td>\n",
       "      <td>0</td>\n",
       "      <td>奥创 弱 爆 弱 爆 弱 爆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...</td>\n",
       "      <td>1</td>\n",
       "      <td>第一集 不同 承上启下 阴郁 严肃 不会 好看 本来 喜欢 漫威 电影 场面 更加 宏大 团...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹</td>\n",
       "      <td>1</td>\n",
       "      <td>激动 友人 说 奥创 毁灭 台北 厚 肩膀 没事 反正 买 两份 旅行 惹</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Star  \\\n",
       "0                                       连奥创都知道整容要去韩国     1   \n",
       "1  一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...     1   \n",
       "2                                       奥创弱爆了弱爆了弱爆了啊     0   \n",
       "3  与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...     1   \n",
       "4      看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹     1   \n",
       "\n",
       "                                   comment_processed  \n",
       "0                                       奥创 知道 整容 韩国   \n",
       "1  一个 没有 黑暗面 值得 信任 第二部 冗长 铺垫 开场 高潮 一直 结束 会 有人 觉得 ...  \n",
       "2                                    奥创 弱 爆 弱 爆 弱 爆   \n",
       "3  第一集 不同 承上启下 阴郁 严肃 不会 好看 本来 喜欢 漫威 电影 场面 更加 宏大 团...  \n",
       "4             激动 友人 说 奥创 毁灭 台北 厚 肩膀 没事 反正 买 两份 旅行 惹   "
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 观察新的数据的格式\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkV7uxjYSnEI"
   },
   "outputs": [],
   "source": [
    "def list_to_str(sentence):\n",
    "  outstr = ''\n",
    "  for word in sentence:\n",
    "      outstr += word\n",
    "      outstr += \" \"\n",
    "  return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBAsed0xSnEJ",
    "outputId": "123b5af7-853c-4d37-f12f-6658943a0959"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bin/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "apply: 100%|██████████| 212506/212506 [00:00<00:00, 548772.23it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc='apply')\n",
    "data['comment_processed'] = data['comment_processed'].progress_apply(list_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KwrarvISnEJ"
   },
   "outputs": [],
   "source": [
    "data.to_csv('data/DMSC_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "E36Boib3SnEJ",
    "outputId": "36584e4e-344d-41d4-8721-7bfeaaa347c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "      <th>comment_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>连奥创都知道整容要去韩国</td>\n",
       "      <td>1</td>\n",
       "      <td>奥创 知道 整容 韩国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...</td>\n",
       "      <td>1</td>\n",
       "      <td>一个 没有 黑暗面 值得 信任 第二部 冗长 铺垫 开场 高潮 一直 结束 会 有人 觉得 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊</td>\n",
       "      <td>0</td>\n",
       "      <td>奥创 弱 爆 弱 爆 弱 爆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...</td>\n",
       "      <td>1</td>\n",
       "      <td>第一集 不同 承上启下 阴郁 严肃 不会 好看 本来 喜欢 漫威 电影 场面 更加 宏大 团...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹</td>\n",
       "      <td>1</td>\n",
       "      <td>激动 友人 说 奥创 毁灭 台北 厚 肩膀 没事 反正 买 两份 旅行 惹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>绝逼不质疑尾灯的导演和编剧水平</td>\n",
       "      <td>1</td>\n",
       "      <td>绝逼 质疑 尾灯 导演 编剧 水平</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>睡着次睡着两次</td>\n",
       "      <td>0</td>\n",
       "      <td>睡着 次 睡着 两次</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>谁再喊我看这种电影我和谁急实在是接受无能</td>\n",
       "      <td>0</td>\n",
       "      <td>喊 这种 电影 急 实在 接受 无能</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>超愉悦以及超满足在历经了第一阶段比漫画更普世的设定融合之后发展到居然出现了不少传统科幻的哲思...</td>\n",
       "      <td>1</td>\n",
       "      <td>超 愉悦 超 满足 历经 漫画 设定 融合 之后 发展 居然 出现 不少 传统 科幻 尾灯 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>观影过程中耳边一直有一种突突突突突的声音我还感慨电影为了让奥创给观众带来紧张感声音上真是下了...</td>\n",
       "      <td>1</td>\n",
       "      <td>观影 过程 中 耳边 一直 一种 突突突 声音 感慨 电影 奥创 观众 带来 紧张感 声音 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>最后大战灾难性得乱到底什么能力完全没明白是巴菲里的其实剧本没那么差美国例外论的主题像是的菜</td>\n",
       "      <td>1</td>\n",
       "      <td>最后 大战 到底 能力 完全 没 明白 里 其实 剧本 没 差 美国 例外 主题 像是 菜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>视觉效果的极限是视觉疲劳</td>\n",
       "      <td>1</td>\n",
       "      <td>视觉效果 极限 视觉 疲劳</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>感觉有略黑暗了点不过还是萌点满满但是一想到就要完结了又心碎了一地</td>\n",
       "      <td>1</td>\n",
       "      <td>感觉 黑暗 点 萌点 满满 想到 完结 心碎 一地</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>妇联成员都只会讲不好笑的笑话唯一加分的是朱莉德培</td>\n",
       "      <td>0</td>\n",
       "      <td>妇联 成员 只会 讲 不好 笑 笑话 唯一 加分</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>只算還的商業片現在這類片型第一品牌就是漫威了熱鬧打鬥大場面人神機甲齊飛各型超級英雄齊聚但講真...</td>\n",
       "      <td>1</td>\n",
       "      <td>商業 第一 品牌 漫威 超級 英雄 真的 劇情 更 這樣 節奏 比差 角色 散 漫威 反派 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>好看好看好看</td>\n",
       "      <td>1</td>\n",
       "      <td>好看 好看 好看</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>难看一笔</td>\n",
       "      <td>0</td>\n",
       "      <td>难看 一笔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>第一部精准的节奏巧妙的悬念和清楚的内心戏不见了或许导演不想把超级英雄打造成战斗机器所以加入了...</td>\n",
       "      <td>1</td>\n",
       "      <td>第一部 精准 节奏 巧妙 悬念 清楚 内心 戏 不见 或许 导演 不想 超级 英雄 造成 战...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>欧洲竟然真的是最早上映啊法国比美国还早一周没怎么看懂的我想找科普说明都不容易嘛我觉得非常一般...</td>\n",
       "      <td>1</td>\n",
       "      <td>欧洲 竟然 真的 最早 上映 法国 美国 早 一周 没 懂 想 找 科普 说明 容易 觉得 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>我是美队的忠实脑残粉</td>\n",
       "      <td>1</td>\n",
       "      <td>美队 忠实 脑残粉</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>辣条电影：</td>\n",
       "      <td>1</td>\n",
       "      <td>电影</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>真不怎么样啊而且觉得影院字幕的翻译是不是有问题啊快银你就这么死了连名字都没能出现过真替你不值...</td>\n",
       "      <td>1</td>\n",
       "      <td>真 不怎么样 觉得 影院 字幕 翻译 是不是 问题 快 银 死 名字 没能 出现 不值 该死...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>没看过前面几部我真心够还好是和一起看的</td>\n",
       "      <td>1</td>\n",
       "      <td>没 看过 前面 几部 真心 够 还好 一起</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>堂堂奥创居然被这么几个复仇者击败回想起漫画的奥创纪元里多超级英雄一起上都挡不住奥创大军真是糟...</td>\n",
       "      <td>1</td>\n",
       "      <td>奥创 居然 几个 复仇者 击败 回 想起 漫画 奥创 纪元 里 超级 英雄 一起 挡不住 奥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>可不可以有点创意啊怎么还是第一部的老套数</td>\n",
       "      <td>1</td>\n",
       "      <td>可不可以 有点 创意 第一部 老</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>拜托不要再拍续集了</td>\n",
       "      <td>0</td>\n",
       "      <td>拜托 不要 拍 续集</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>竟然没标吗</td>\n",
       "      <td>1</td>\n",
       "      <td>竟然</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>开片开始故作搞笑真的好反感后来随着剧情的深入偶尔的还戏谑还是没那么违和总体来说也确实太泛滥了...</td>\n",
       "      <td>1</td>\n",
       "      <td>开片 故作 搞笑 真的 反感 后来 剧情 深入 偶尔 戏谑 没 违和 总体 确实 太 泛滥 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>景观电影又一部大家打浩克和钢铁侠打队长和黑寡妇和鹰眼打大家打中间插点过渡的戏份就这么个意思看...</td>\n",
       "      <td>1</td>\n",
       "      <td>景观 电影 一部 钢铁 侠 队长 黑寡妇 鹰眼 中间 过渡 戏份 意思 完 脑子里 剩 不下...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>跟第一部比差很远人太多了没有故事黑寡妇跟浩克的感情真心不能理解后面出来的很厉害的人不知道干嘛的失望</td>\n",
       "      <td>0</td>\n",
       "      <td>第一部 比差 远 人太多 没有 故事 黑寡妇 感情 真心 不能 理解 后面 厉害 知道 干嘛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>什么情况</td>\n",
       "      <td>1</td>\n",
       "      <td>情况</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>就是不感兴趣特别是没看又是中文配音</td>\n",
       "      <td>0</td>\n",
       "      <td>不感兴趣 特别 没 中文 配音</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>这也叫反派最后一点点期待的超级无敌大反派居然从良了</td>\n",
       "      <td>0</td>\n",
       "      <td>反派 最后 一点点 期待 超级 无敌 反派 居然 从良</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>虽然这个系列的漫画的电影是零星拼凑得看着的但总体感觉还是这部挺不错的入场时的大动作是最震撼的...</td>\n",
       "      <td>1</td>\n",
       "      <td>系列 漫画 电影 零星 拼凑 看着 总体 感觉 这部 挺不错 入场 时 动作 震撼 后面 每...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>角色更多了但是要涵盖整个漫威宇宙显得剧情变得尤为的复杂</td>\n",
       "      <td>1</td>\n",
       "      <td>角色 更 涵盖 整个 漫威 宇宙 显得 剧情 变得 尤为 复杂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>没有复联好看因为反派太丑</td>\n",
       "      <td>1</td>\n",
       "      <td>没有 复联 好看 反派 太丑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>作为铁粉我觉得就值星</td>\n",
       "      <td>1</td>\n",
       "      <td>铁粉 觉得 值星</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>脑残粉我还是很喜欢曼威系列超级英雄从头打到尾的混战的这集的浩克大战大钢铁侠还是很过瘾其实最喜...</td>\n",
       "      <td>1</td>\n",
       "      <td>脑残粉 喜欢 系列 超级 英雄 从头 打到 尾 混战 这集 浩克 大战 钢铁 侠 过瘾 其实...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>有点理解这部评价为什么上不去真的太散了前半段东一榔头西一棒子不过复联一直都是个松散联盟本集倒...</td>\n",
       "      <td>1</td>\n",
       "      <td>有点 理解 这部 评价 真的 太散 前半段 棒子 复联 一直 松散 联盟 倒 秉承 下来 妮...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>精彩的地方不过十几秒</td>\n",
       "      <td>1</td>\n",
       "      <td>精彩 地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>众基友替钢铁侠擦屁股</td>\n",
       "      <td>1</td>\n",
       "      <td>钢铁 侠 擦屁股</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>审美疲劳</td>\n",
       "      <td>1</td>\n",
       "      <td>审美疲劳</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>最近没得空看片结果碰到一坨渣也看不出毛效果</td>\n",
       "      <td>0</td>\n",
       "      <td>最近 没 看片 碰到 一坨 渣 看不出 毛 效果</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>略散略长</td>\n",
       "      <td>1</td>\n",
       "      <td>略</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>睡了半场</td>\n",
       "      <td>0</td>\n",
       "      <td>睡 半场</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>在全州电影节间隙看了依旧眼花缭乱爽歪歪首尔和秀贤的戏份比想的多也并不生硬但和雷神感情线完全没...</td>\n",
       "      <td>1</td>\n",
       "      <td>电影节 间隙 依旧 眼花缭乱 首尔 戏份 生硬 雷神 感情 线 完全 没有 回应 鹰眼 家庭...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>带爸妈看我爸居然没睡着全家人被安插在打斗环节中的冷笑话搞得一头雾水着实觉得奥创羞羞羞这么弱好...</td>\n",
       "      <td>0</td>\n",
       "      <td>带 爸妈 爸 居然 没 睡着 全家人 打斗 环节 中 冷笑 话 搞 一头雾水 着实 觉得 奥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>纠结什么剧情啊什么可不可能啊的为什么就不明白看的就是一个爽呢</td>\n",
       "      <td>1</td>\n",
       "      <td>纠结 剧情 明白 一个 爽</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>还好只是时光荏苒已经不再是粉了</td>\n",
       "      <td>1</td>\n",
       "      <td>还好 已经 不再 粉</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>一开始说要看我是拒绝的我讨厌戴着夹片看电影的感觉累但是同时没有其他值得看的好片子我就看了呃看...</td>\n",
       "      <td>1</td>\n",
       "      <td>说 拒绝 讨厌 戴着 电影 感觉 累 没有 值得 片子 看个 热闹 片子</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                            Comment  Star  \\\n",
       "0            0                                       连奥创都知道整容要去韩国     1   \n",
       "1            1  一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...     1   \n",
       "2            2                                       奥创弱爆了弱爆了弱爆了啊     0   \n",
       "3            3  与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...     1   \n",
       "4            4      看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹     1   \n",
       "5            5                                    绝逼不质疑尾灯的导演和编剧水平     1   \n",
       "6            6                                            睡着次睡着两次     0   \n",
       "7            7                               谁再喊我看这种电影我和谁急实在是接受无能     0   \n",
       "8            8  超愉悦以及超满足在历经了第一阶段比漫画更普世的设定融合之后发展到居然出现了不少传统科幻的哲思...     1   \n",
       "9            9  观影过程中耳边一直有一种突突突突突的声音我还感慨电影为了让奥创给观众带来紧张感声音上真是下了...     1   \n",
       "10          10      最后大战灾难性得乱到底什么能力完全没明白是巴菲里的其实剧本没那么差美国例外论的主题像是的菜     1   \n",
       "11          11                                       视觉效果的极限是视觉疲劳     1   \n",
       "12          12                   感觉有略黑暗了点不过还是萌点满满但是一想到就要完结了又心碎了一地     1   \n",
       "13          13                           妇联成员都只会讲不好笑的笑话唯一加分的是朱莉德培     0   \n",
       "14          14  只算還的商業片現在這類片型第一品牌就是漫威了熱鬧打鬥大場面人神機甲齊飛各型超級英雄齊聚但講真...     1   \n",
       "15          15                                             好看好看好看     1   \n",
       "16          16                                               难看一笔     0   \n",
       "17          17  第一部精准的节奏巧妙的悬念和清楚的内心戏不见了或许导演不想把超级英雄打造成战斗机器所以加入了...     1   \n",
       "18          18  欧洲竟然真的是最早上映啊法国比美国还早一周没怎么看懂的我想找科普说明都不容易嘛我觉得非常一般...     1   \n",
       "19          19                                         我是美队的忠实脑残粉     1   \n",
       "20          20                                              辣条电影：     1   \n",
       "21          21  真不怎么样啊而且觉得影院字幕的翻译是不是有问题啊快银你就这么死了连名字都没能出现过真替你不值...     1   \n",
       "22          22                                没看过前面几部我真心够还好是和一起看的     1   \n",
       "23          23  堂堂奥创居然被这么几个复仇者击败回想起漫画的奥创纪元里多超级英雄一起上都挡不住奥创大军真是糟...     1   \n",
       "24          24                               可不可以有点创意啊怎么还是第一部的老套数     1   \n",
       "25          25                                          拜托不要再拍续集了     0   \n",
       "26          26                                              竟然没标吗     1   \n",
       "27          27  开片开始故作搞笑真的好反感后来随着剧情的深入偶尔的还戏谑还是没那么违和总体来说也确实太泛滥了...     1   \n",
       "28          28  景观电影又一部大家打浩克和钢铁侠打队长和黑寡妇和鹰眼打大家打中间插点过渡的戏份就这么个意思看...     1   \n",
       "29          29  跟第一部比差很远人太多了没有故事黑寡妇跟浩克的感情真心不能理解后面出来的很厉害的人不知道干嘛的失望     0   \n",
       "30          30                                               什么情况     1   \n",
       "31          31                                  就是不感兴趣特别是没看又是中文配音     0   \n",
       "32          32                          这也叫反派最后一点点期待的超级无敌大反派居然从良了     0   \n",
       "33          33  虽然这个系列的漫画的电影是零星拼凑得看着的但总体感觉还是这部挺不错的入场时的大动作是最震撼的...     1   \n",
       "35          35                        角色更多了但是要涵盖整个漫威宇宙显得剧情变得尤为的复杂     1   \n",
       "37          37                                       没有复联好看因为反派太丑     1   \n",
       "38          38                                         作为铁粉我觉得就值星     1   \n",
       "39          39  脑残粉我还是很喜欢曼威系列超级英雄从头打到尾的混战的这集的浩克大战大钢铁侠还是很过瘾其实最喜...     1   \n",
       "40          40  有点理解这部评价为什么上不去真的太散了前半段东一榔头西一棒子不过复联一直都是个松散联盟本集倒...     1   \n",
       "41          41                                         精彩的地方不过十几秒     1   \n",
       "42          42                                         众基友替钢铁侠擦屁股     1   \n",
       "43          43                                               审美疲劳     1   \n",
       "44          44                              最近没得空看片结果碰到一坨渣也看不出毛效果     0   \n",
       "45          45                                               略散略长     1   \n",
       "46          46                                               睡了半场     0   \n",
       "47          47  在全州电影节间隙看了依旧眼花缭乱爽歪歪首尔和秀贤的戏份比想的多也并不生硬但和雷神感情线完全没...     1   \n",
       "49          49  带爸妈看我爸居然没睡着全家人被安插在打斗环节中的冷笑话搞得一头雾水着实觉得奥创羞羞羞这么弱好...     0   \n",
       "50          50                     纠结什么剧情啊什么可不可能啊的为什么就不明白看的就是一个爽呢     1   \n",
       "51          51                                    还好只是时光荏苒已经不再是粉了     1   \n",
       "52          52  一开始说要看我是拒绝的我讨厌戴着夹片看电影的感觉累但是同时没有其他值得看的好片子我就看了呃看...     1   \n",
       "\n",
       "                                    comment_processed  \n",
       "0                                        奥创 知道 整容 韩国   \n",
       "1   一个 没有 黑暗面 值得 信任 第二部 冗长 铺垫 开场 高潮 一直 结束 会 有人 觉得 ...  \n",
       "2                                     奥创 弱 爆 弱 爆 弱 爆   \n",
       "3   第一集 不同 承上启下 阴郁 严肃 不会 好看 本来 喜欢 漫威 电影 场面 更加 宏大 团...  \n",
       "4              激动 友人 说 奥创 毁灭 台北 厚 肩膀 没事 反正 买 两份 旅行 惹   \n",
       "5                                  绝逼 质疑 尾灯 导演 编剧 水平   \n",
       "6                                         睡着 次 睡着 两次   \n",
       "7                                 喊 这种 电影 急 实在 接受 无能   \n",
       "8   超 愉悦 超 满足 历经 漫画 设定 融合 之后 发展 居然 出现 不少 传统 科幻 尾灯 ...  \n",
       "9   观影 过程 中 耳边 一直 一种 突突突 声音 感慨 电影 奥创 观众 带来 紧张感 声音 ...  \n",
       "10     最后 大战 到底 能力 完全 没 明白 里 其实 剧本 没 差 美国 例外 主题 像是 菜   \n",
       "11                                     视觉效果 极限 视觉 疲劳   \n",
       "12                         感觉 黑暗 点 萌点 满满 想到 完结 心碎 一地   \n",
       "13                          妇联 成员 只会 讲 不好 笑 笑话 唯一 加分   \n",
       "14  商業 第一 品牌 漫威 超級 英雄 真的 劇情 更 這樣 節奏 比差 角色 散 漫威 反派 ...  \n",
       "15                                          好看 好看 好看   \n",
       "16                                             难看 一笔   \n",
       "17  第一部 精准 节奏 巧妙 悬念 清楚 内心 戏 不见 或许 导演 不想 超级 英雄 造成 战...  \n",
       "18  欧洲 竟然 真的 最早 上映 法国 美国 早 一周 没 懂 想 找 科普 说明 容易 觉得 ...  \n",
       "19                                         美队 忠实 脑残粉   \n",
       "20                                                电影   \n",
       "21  真 不怎么样 觉得 影院 字幕 翻译 是不是 问题 快 银 死 名字 没能 出现 不值 该死...  \n",
       "22                             没 看过 前面 几部 真心 够 还好 一起   \n",
       "23  奥创 居然 几个 复仇者 击败 回 想起 漫画 奥创 纪元 里 超级 英雄 一起 挡不住 奥...  \n",
       "24                                  可不可以 有点 创意 第一部 老   \n",
       "25                                        拜托 不要 拍 续集   \n",
       "26                                                竟然   \n",
       "27  开片 故作 搞笑 真的 反感 后来 剧情 深入 偶尔 戏谑 没 违和 总体 确实 太 泛滥 ...  \n",
       "28  景观 电影 一部 钢铁 侠 队长 黑寡妇 鹰眼 中间 过渡 戏份 意思 完 脑子里 剩 不下...  \n",
       "29  第一部 比差 远 人太多 没有 故事 黑寡妇 感情 真心 不能 理解 后面 厉害 知道 干嘛...  \n",
       "30                                                情况   \n",
       "31                                   不感兴趣 特别 没 中文 配音   \n",
       "32                       反派 最后 一点点 期待 超级 无敌 反派 居然 从良   \n",
       "33  系列 漫画 电影 零星 拼凑 看着 总体 感觉 这部 挺不错 入场 时 动作 震撼 后面 每...  \n",
       "35                   角色 更 涵盖 整个 漫威 宇宙 显得 剧情 变得 尤为 复杂   \n",
       "37                                    没有 复联 好看 反派 太丑   \n",
       "38                                          铁粉 觉得 值星   \n",
       "39  脑残粉 喜欢 系列 超级 英雄 从头 打到 尾 混战 这集 浩克 大战 钢铁 侠 过瘾 其实...  \n",
       "40  有点 理解 这部 评价 真的 太散 前半段 棒子 复联 一直 松散 联盟 倒 秉承 下来 妮...  \n",
       "41                                             精彩 地方   \n",
       "42                                          钢铁 侠 擦屁股   \n",
       "43                                              审美疲劳   \n",
       "44                          最近 没 看片 碰到 一坨 渣 看不出 毛 效果   \n",
       "45                                                 略   \n",
       "46                                              睡 半场   \n",
       "47  电影节 间隙 依旧 眼花缭乱 首尔 戏份 生硬 雷神 感情 线 完全 没有 回应 鹰眼 家庭...  \n",
       "49  带 爸妈 爸 居然 没 睡着 全家人 打斗 环节 中 冷笑 话 搞 一头雾水 着实 觉得 奥...  \n",
       "50                                     纠结 剧情 明白 一个 爽   \n",
       "51                                        还好 已经 不再 粉   \n",
       "52              说 拒绝 讨厌 戴着 电影 感觉 累 没有 值得 片子 看个 热闹 片子   "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_processed = pd.read_csv('/content/drive/MyDrive/Greedy/project1-douban/data/DMSC_processed.csv')\n",
    "data_processed = pd.read_csv('data/DMSC_processed.csv')\n",
    "data_without_NaN = data_processed.dropna(axis=0)\n",
    "data_without_NaN.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKNu5xFn4_6Q"
   },
   "source": [
    "### 2. 把文本分为训练集和测试集\n",
    "选择语料库中的20%作为测试数据，剩下的作为训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "DtfDUhvHCAbu"
   },
   "outputs": [],
   "source": [
    "# TODO5: 把数据分为训练集和测试集. comments_train（list)保存用于训练的文本，comments_test(list)保存用于测试的文本。 y_train, y_test是对应的标签（0、1）\n",
    "test_ratio = 0.2\n",
    "train_num = int((1-test_ratio)*len(data_without_NaN['comment_processed']))\n",
    "\n",
    "comment_list = []\n",
    "for line in data_without_NaN['Comment']:\n",
    "  comment_list.append([line])\n",
    "\n",
    "tokenized_comm_list = []\n",
    "for line in data_without_NaN['comment_processed']:\n",
    "  tokenized_comm_list.append([line])\n",
    "\n",
    "star_list = []\n",
    "for line in data_without_NaN['Star']:\n",
    "  star_list.append(line)\n",
    "len(tokenized_comm_list)\n",
    "\n",
    "# Label\n",
    "y_train, y_test  = star_list[:train_num], star_list[train_num:]\n",
    "\n",
    "# Tfidf训练集和测试集\n",
    "tfidf_train = []\n",
    "tfidf_test = []\n",
    "\n",
    "for i in range(train_num):\n",
    "  tfidf_train = tfidf_train + tokenized_comm_list[i]\n",
    "for i in range(train_num ,len(tokenized_comm_list)):\n",
    "  tfidf_test = tfidf_test + tokenized_comm_list[i]\n",
    "\n",
    "# Word2Vec训练集和测试集\n",
    "word2veclist_train = []\n",
    "word2veclist_test = []\n",
    "\n",
    "for i in range(train_num):\n",
    "  word2veclist_train.append(tokenized_comm_list[i])\n",
    "for i in range(train_num ,len(tokenized_comm_list)):\n",
    "  word2veclist_test.append(tokenized_comm_list[i])\n",
    "\n",
    "# Bert-Embedding训练集和测试集\n",
    "bert_comments_train, bert_comments_test = comment_list[:train_num], comment_list[train_num:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCA6GyBu4_6R"
   },
   "source": [
    "### 3. 把文本转换成向量的形式\n",
    "\n",
    "在这个部分我们会采用三种不同的方式:\n",
    "- 使用tf-idf向量\n",
    "- 使用word2vec\n",
    "- 使用bert向量\n",
    "\n",
    "转换成向量之后，我们接着做模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVDOLJ1A4_6R"
   },
   "source": [
    "#### 任务6：把文本转换成tf-idf向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ir0MHuQ04_6R",
    "outputId": "de59c46c-3b09-4d08-f51c-b00766f47f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165972, 14235) (41494, 14235)\n"
     ]
    }
   ],
   "source": [
    "# TODO6: 把训练文本和测试文本转换成tf-idf向量。使用sklearn的feature_extraction.text.TfidfTransformer模块\n",
    "#    请留意fit_transform和transform之间的区别。 常见的错误是在训练集和测试集上都使用 fit_transform，需要避免！ \n",
    "#    另外，可以留意一下结果是否为稀疏矩阵\n",
    "from  sklearn.feature_extraction.text  import  CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer#TfidfVectorizer类可以帮助完成向量化、TF-IDF和标准化三步。当然，还可以帮我们处理停用词。\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_tr = transformer.fit_transform(vectorizer.fit_transform(tfidf_train))\n",
    "\n",
    "tfidf_te = transformer.transform(vectorizer.transform(tfidf_test))\n",
    "\n",
    "print (tfidf_tr.shape, tfidf_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ8ZnzJq4_6R"
   },
   "source": [
    "#### 任务7：把文本转换成word2vec向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "mizVbZAt4_6R"
   },
   "outputs": [],
   "source": [
    "# 由于训练出一个高效的word2vec词向量往往需要非常大的语料库与计算资源，所以我们通常不自己训练Wordvec词向量，而直接使用网上开源的已训练好的词向量。\n",
    "# data/sgns.zhihu.word是从https://github.com/Embedding/Chinese-Word-Vectors下载到的预训练好的中文词向量文件\n",
    "# 使用KeyedVectors.load_word2vec_format()函数加载预训练好的词向量文件\n",
    "model = KeyedVectors.load_word2vec_format('data/sgns.zhihu.word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eHNQhuk4_6R",
    "outputId": "d9842cbc-ba2f-4457-a73f-0c3c18740445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预训练词向量使用举例\n",
    "model['今天'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JOFmpT9xj5sD"
   },
   "outputs": [],
   "source": [
    "vocabulary = model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "f94FF-gM4_6R"
   },
   "outputs": [],
   "source": [
    "# TODO7: 对于每个句子，生成句子的向量。具体的做法是：包含在句子中的所有单词的向量做平均。\n",
    "\n",
    "def sent2vec(sentence_list):\n",
    "    \n",
    "    words = sentence_list[0].strip().split(' ')\n",
    "    sent_vec = np.zeros([300,])\n",
    "    for item in words:\n",
    "        if item not in vocabulary:\n",
    "            continue\n",
    "        else:\n",
    "            sent_vec = sent_vec + model[item]\n",
    "    sent_vec = sent_vec/len(words)\n",
    "    \n",
    "    return sent_vec\n",
    "def comm_list2vec(comm_list):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i in tqdm(range(len(comm_list))): \n",
    "        result.append(sent2vec(comm_list[i]))\n",
    "    \n",
    "    return np.array(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkKT1mGNq9yi",
    "outputId": "778569a1-c3d3-436c-8379-239f574978b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165972/165972 [00:04<00:00, 33469.76it/s]\n",
      "100%|██████████| 41494/41494 [00:01<00:00, 36562.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165972, 300) (41494, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec_train = comm_list2vec(word2veclist_train)\n",
    "\n",
    "word2vec_test = comm_list2vec(word2veclist_test)\n",
    "\n",
    "print (word2vec_train.shape, word2vec_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tfsGnFd4_6S"
   },
   "source": [
    "#### 任务8：把文本转换成bert向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOj4WKiE4_6S",
    "outputId": "2ca446b3-9148-4add-e8b8-ad0cc5ee1d84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gluonnlp/model/bert.py:693: UserWarning: wiki_cn/wiki_multilingual will be deprecated. Please use wiki_cn_cased/wiki_multilingual_uncased instead.\n",
      "  warnings.warn('wiki_cn/wiki_multilingual will be deprecated.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /root/.mxnet/models/wiki_cn-a1e06f8e.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/wiki_cn-a1e06f8e.zip...\n",
      "Downloading /root/.mxnet/models/bert_12_768_12_wiki_cn-885ebb9a.zip5d2acb37-c6c9-4258-b068-84d36d5877b8 from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_wiki_cn-885ebb9a.zip...\n"
     ]
    }
   ],
   "source": [
    "# 导入gpu版本的bert embedding预训练的模型。\n",
    "# 若没有gpu，则ctx可使用其默认值cpu(0)。但使用cpu会使程序运行的时间变得非常慢\n",
    "# 若之前没有下载过bert embedding预训练的模型，执行此句时会花费一些时间来下载预训练的模型\n",
    "ctx = mxnet.gpu()\n",
    "\n",
    "embedding = BertEmbedding(ctx = ctx, model='bert_12_768_12', dataset_name='wiki_cn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-ycPB0aRSnEK"
   },
   "outputs": [],
   "source": [
    "def ger_sen_vec(word_vec):\n",
    "\n",
    "  vec_array = np.zeros([768,])\n",
    "  for i in range(1, len(word_vec[0][1])-1):\n",
    "    vec_array = vec_array + np.array(word_vec[0][1][i])\n",
    "  new_vec = vec_array/(len(word_vec[0][1]) - 2)\n",
    "  return new_vec\n",
    "\n",
    "def get_bert_vec (sentences):\n",
    "  bert_train = []\n",
    "  for i in tqdm(range(len(sentences))):\n",
    "    restu1 = embedding(sentences[i])\n",
    "    bert_vec = ger_sen_vec(restu1)\n",
    "    bert_train.append(bert_vec)\n",
    "  return np.array(bert_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lj5JxBwTSnEK",
    "outputId": "b6458f83-e3fb-41e9-8fc4-e4ccf4010492"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165972/165972 [1:58:19<00:00, 23.38it/s]\n",
      "100%|██████████| 41494/41494 [28:59<00:00, 23.86it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_train = get_bert_vec(bert_comments_train)\n",
    "bert_test = get_bert_vec(bert_comments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4eyytawaPZU",
    "outputId": "8bae8897-6550-4358-f259-e560689e7d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165972, 768) (41494, 768)\n"
     ]
    }
   ],
   "source": [
    "print (bert_train.shape, bert_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdvQc9i74_6S",
    "outputId": "bb259d6d-e578-4dc2-e4f5-753015281fd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165972, 14235) (41494, 14235)\n",
      "(165972, 300) (41494, 300)\n",
      "(165972, 768) (41494, 768)\n"
     ]
    }
   ],
   "source": [
    "print (tfidf_tr.shape, tfidf_te.shape)\n",
    "print (word2vec_train.shape, word2vec_test.shape)\n",
    "print (bert_train.shape, bert_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtQvWqBb4_6S"
   },
   "source": [
    "### 4. 训练模型以及评估\n",
    "对如上三种不同的向量表示法，分别训练逻辑回归模型，需要做：\n",
    "- 搭建模型\n",
    "- 训练模型（并做交叉验证）\n",
    "- 输出最好的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "_kclVpkt4_6S"
   },
   "outputs": [],
   "source": [
    "# 导入逻辑回归的包\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxPemzla4_6S"
   },
   "source": [
    "#### 任务9：使用tf-idf，并结合逻辑回归训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGUO8gal4_6T",
    "outputId": "ee3b6742-14ff-48ff-a23d-d9b27e66bc2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1}\n",
      "0.8213131030500079\n",
      "LogisticRegression(C=0.1, max_iter=500, solver='sag')\n"
     ]
    }
   ],
   "source": [
    "# TODO9: 使用tf-idf + 逻辑回归训练模型，需要用gridsearchCV做交叉验证，并选择最好的超参数\n",
    "\n",
    "tfidf_lr = LR(penalty=\"l2\",solver = 'sag',max_iter = 500)\n",
    "\n",
    "param = {\n",
    "    'C' : [0.09,0.1,0.15,0.2,0.4,0.5,0.7]\n",
    "}\n",
    "grid = GridSearchCV(estimator = tfidf_lr, \n",
    "                    param_grid = param, \n",
    "                    cv = 5\n",
    "                   )\n",
    "\n",
    "grid.fit(tfidf_tr,y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "zQO7vwwJJVoq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF LR test accuracy 0.9375331373210585\n",
      "TF-IDF LR test F1_score 0.6337616919812451\n"
     ]
    }
   ],
   "source": [
    "tfidf_lr = LR(C = 0.1, max_iter = 500, solver = 'sag')\n",
    "tfidf_lr.fit(tfidf_tr,y_train)\n",
    "tf_idf_y_pred = tfidf_lr.predict(tfidf_te)\n",
    "print('TF-IDF LR test accuracy %s' % metrics.accuracy_score(y_test, tf_idf_y_pred))\n",
    "#逻辑回归模型在测试集上的F1_Score\n",
    "print('TF-IDF LR test F1_score %s' % metrics.f1_score(y_test, tf_idf_y_pred,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gj3PAsNo4_6T"
   },
   "source": [
    "#### 任务10：使用word2vec，并结合逻辑回归训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "3b7Qwnh74_6T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.03}\n",
      "0.8232773579191386\n",
      "LogisticRegression(C=0.03, max_iter=500, solver='sag')\n"
     ]
    }
   ],
   "source": [
    "# TODO10: 使用word2vec + 逻辑回归训练模型，需要用gridsearchCV做交叉验证，并选择最好的超参数\n",
    "\n",
    "word2vec_lr = LR(penalty=\"l2\",solver = 'sag',max_iter = 500)\n",
    "\n",
    "param = {\n",
    "    'C' : [0.01,0.03,0.06,0.08,0.09,0.1,0.15,0.2,0.4,0.5]\n",
    "}\n",
    "word2vec_grid = GridSearchCV(estimator = tfidf_lr, \n",
    "                    param_grid = param, \n",
    "                    cv = 5 \n",
    "                   )\n",
    "\n",
    "word2vec_grid.fit(word2vec_train,y_train)\n",
    "print(word2vec_grid.best_params_)\n",
    "print(word2vec_grid.best_score_)\n",
    "print(word2vec_grid.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OJ7x_k9gq4w",
    "outputId": "7eb9d834-9536-445b-9516-78b75a146dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec LR test accuracy 0.923386513712826\n",
      "Word2vec LR test F1_score 0.6444193008059403\n"
     ]
    }
   ],
   "source": [
    "print('Word2vec LR test accuracy %s' % metrics.accuracy_score(y_test, word2vec_y_pred))\n",
    "#逻辑回归模型在测试集上的F1_Score\n",
    "print('Word2vec LR test F1_score %s' % metrics.f1_score(y_test, word2vec_y_pred,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETTKgvPR4_6T"
   },
   "source": [
    "#### 任务11：使用bert，并结合逻辑回归训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jfm6FsUog7Z2",
    "outputId": "2594903e-7a93-413b-b997-7f4e1893efd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.09}\n",
      "0.814830182662399\n",
      "LogisticRegression(C=0.09, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='sag', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# TODO11: 使用bert + 逻辑回归训练模型，需要用gridsearchCV做交叉验证，并选择最好的超参数\n",
    "bert_lr = LR(penalty=\"l2\",max_iter = 500,solver = 'sag')\n",
    "\n",
    "param = {\n",
    "    'C' : [0.09,0.1,0.2,0.4,0.7]\n",
    "}\n",
    "bert_grid = GridSearchCV(estimator = bert_lr, \n",
    "                    param_grid = param, \n",
    "                    cv = 5 \n",
    "                   )\n",
    "\n",
    "bert_grid.fit(bert_train,y_train)\n",
    "print(bert_grid.best_params_)\n",
    "print(bert_grid.best_score_)\n",
    "print(bert_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGpP6LTO4_6T",
    "outputId": "d99fdba4-a8ef-44c7-f5e3-ec214bca3501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert LR test accuracy 0.9207355280281486\n",
      "Bert LR test F1_score 0.6131981416847541\n"
     ]
    }
   ],
   "source": [
    "bert_lr = LR(penalty=\"l2\",solver=\"sag\",C=0.09,max_iter=500)\n",
    "bert_lr.fit(bert_train,y_train)\n",
    " \n",
    "bert_y_pred = bert_lr.predict(bert_test)\n",
    "\n",
    "print('Bert LR test accuracy %s' % metrics.accuracy_score(y_test, bert_y_pred))\n",
    "#逻辑回归模型在测试集上的F1_Score\n",
    "print('Bert LR test F1_score %s' % metrics.f1_score(y_test, bert_y_pred,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOpVGAQf4_6T"
   },
   "source": [
    "#### 任务12：对于以上结果请做一下简单的总结，按照1，2，3，4提取几个关键点，包括：\n",
    "- 结果说明什么问题？\n",
    "- 接下来如何提高？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TQHHN0H4_6T"
   },
   "source": [
    "1.jieba分词有时候会将一些不该分开的词汇分开，例如它将“不值得”分成“不”，“值得”，而后“不”被当作停用词去除，影响了整体的语义。分词词库还得根据应用场景做对应修改。 \n",
    "\n",
    "2.在word2vec时才发现有空文本存在，应该在处理数据时就该发现处理。\n",
    "\n",
    "3.通过逻辑回归建模三种word embedding的准确率差不多，在测试集准确率都能达到92%，但是f1不是很高，说明2分类sample不均匀。\n",
    "\n",
    "4.分词结果有些词汇不在word2vec的词库中，我选择了直接将该词跳过，不做word3vec，有待改进。\n",
    "\n",
    "5.可以使用更复杂的模型分类，如神经网络来提高准确率。\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "douban_starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
